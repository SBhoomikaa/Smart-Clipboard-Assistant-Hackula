{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGo5X7UFYS-R"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers tensorflow pandas scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def validate_model(model_path, csv_path, text_column, label_column, max_length):\n",
        "    \"\"\"\n",
        "    Validate a TFLite model for contact information classification\n",
        "    \"\"\"\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(csv_path)\n",
        "    texts = df[text_column].values\n",
        "    y_test = df[label_column].values\n",
        "\n",
        "    # Map labels to integers\n",
        "    label_map = {'address': 0, 'phone': 1, 'email': 2, 'url': 3}\n",
        "    y_test = np.array([label_map[label.lower()] for label in y_test])\n",
        "\n",
        "    print(f\"Loaded {len(texts)} samples from CSV\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    print(\"DistilBERT tokenizer loaded\")\n",
        "\n",
        "    # Load TFLite model\n",
        "    print(\"Loading TFLite model...\")\n",
        "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "    print(\"Allocating tensors...\")\n",
        "    interpreter.allocate_tensors()\n",
        "    print(\"Model ready\")\n",
        "\n",
        "    # Get input and output details\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    print(f\"\\nModel expects {len(input_details)} inputs:\")\n",
        "    for detail in input_details:\n",
        "        print(f\"  - {detail['name']}: shape {detail['shape']}\")\n",
        "\n",
        "    # Run inference on all samples\n",
        "    y_pred = []\n",
        "    for i, text in enumerate(texts):\n",
        "        if i % 100 == 0 and i > 0:\n",
        "            print(f\"Processed {i}/{len(texts)} samples...\")\n",
        "\n",
        "        # Tokenize input\n",
        "        encoded = tokenizer(text,\n",
        "                          max_length=max_length,\n",
        "                          padding='max_length',\n",
        "                          truncation=True,\n",
        "                          return_tensors='np')\n",
        "\n",
        "        input_ids = encoded['input_ids'].astype(np.int32)\n",
        "        attention_mask = encoded['attention_mask'].astype(np.int32)\n",
        "        segment_ids = np.zeros_like(input_ids, dtype=np.int32)\n",
        "\n",
        "        # Set input tensors\n",
        "        interpreter.set_tensor(input_details[0]['index'], input_ids)\n",
        "        interpreter.set_tensor(input_details[1]['index'], attention_mask)\n",
        "        interpreter.set_tensor(input_details[2]['index'], segment_ids)\n",
        "\n",
        "        # Run inference\n",
        "        interpreter.invoke()\n",
        "\n",
        "        # Get prediction\n",
        "        output = interpreter.get_tensor(output_details[0]['index'])\n",
        "        y_pred.append(np.argmax(output[0]))\n",
        "\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"VALIDATION RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(\"-\"*60)\n",
        "    print(\"Rows = Actual | Columns = Predicted\")\n",
        "    print(\"\\n           URL    Email    Phone  Address\")\n",
        "\n",
        "    class_names = ['URL     ', 'Email   ', 'Phone   ', 'Address ']\n",
        "    for i, row in enumerate(cm):\n",
        "        print(f\"{class_names[i]} {row[0]:6d} {row[1]:6d} {row[2]:6d} {row[3]:8d}\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_score': f1_macro,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "# Run validation with files from Google Drive\n",
        "# Update these paths to match where you uploaded your files in Google Drive\n",
        "results = validate_model(\n",
        "    model_path=\"/content/drive/MyDrive/distilbert_with_metaadata.tflite\",  # Update this path\n",
        "    csv_path=\"/content/drive/MyDrive/validation_contacts_dataset.csv\",  # Update this path\n",
        "    text_column='text',\n",
        "    label_column='label',\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "# Display final results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Final Accuracy: {results['accuracy']:.4f}\")\n",
        "print(f\"Final F1 Score: {results['f1_score']:.4f}\")"
      ]
    }
  ]
}